{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c2a2dbf",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/g4merz/.conda/envs/deepdisctest/lib/python3.9/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "# Some basic setup:\n",
    "# Setup detectron2 logger\n",
    "import detectron2\n",
    "from detectron2.utils.logger import setup_logger\n",
    "setup_logger()\n",
    "\n",
    "# import some common libraries\n",
    "import numpy as np\n",
    "import os, json, cv2, random\n",
    "#from google.colab.patches import cv2_imshow\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# import some common detectron2 utilities\n",
    "from detectron2 import model_zoo\n",
    "from detectron2.engine import DefaultPredictor\n",
    "from detectron2.config import get_cfg\n",
    "from detectron2.utils.visualizer import Visualizer\n",
    "from detectron2.data import MetadataCatalog, DatasetCatalog\n",
    "from detectron2.data import build_detection_train_loader\n",
    "from detectron2.engine import DefaultTrainer\n",
    "from detectron2.engine import SimpleTrainer\n",
    "from detectron2.engine import HookBase\n",
    "from typing import Dict, List, Optional\n",
    "import detectron2.solver as solver\n",
    "import detectron2.modeling as modeler\n",
    "import detectron2.data as data\n",
    "import detectron2.data.transforms as T\n",
    "import detectron2.checkpoint as checkpointer\n",
    "from detectron2.data import detection_utils as utils\n",
    "import weakref\n",
    "import copy\n",
    "import torch\n",
    "import time\n",
    "\n",
    "from astrodet import astrodet as toolkit\n",
    "from astrodet import detectron as detectron_addons\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2fad3be",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print the versions to test the imports and so we know what works\n",
    "print(detectron2.__version__)\n",
    "print(np.__version__)\n",
    "print(cv2.__version__)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28b2c4ad-3932-4a06-9166-c60fe4b53e5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prettify the plotting\n",
    "from astrodet.astrodet import set_mpl_style\n",
    "set_mpl_style()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "035b6246",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Register HSC training data\n",
    "\n",
    "First, format the HSC data using training_data.ipynb.  It will need to be partitioned in \"train, test and val\" directories\n",
    "\n",
    "The file metadata for each dataset is specified with the `filesnames_dict`. We will specify the filters first, then populate the filenames in the dataset directory.\n",
    "\n",
    "For a custom dataset, this dictionary needs to be populated correctly for your data.\n",
    "\n",
    "You will need to change directory paths!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "055b5ccc-fbf8-4960-bb7c-7d261fc20474",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "dirpath = '/home/shared/hsc/HSC/HSC_DR3/data/'\n",
    "output_dir = './output/hsc'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6958fcd1-5ce0-43f0-9eee-f2d877148036",
   "metadata": {},
   "outputs": [],
   "source": [
    "#this block is for debug purposes, set to -1 to include every sample\n",
    "sampleNumbers = 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee4286c6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from detectron2.structures import BoxMode\n",
    "from astropy.io import fits\n",
    "import glob\n",
    "#Yufeng Jun19 add test here\n",
    "dataset_names = ['train','test'] #, 'val'] # train\n",
    "filenames_dict_list = [] # List holding filenames_dict for each dataset\n",
    "\n",
    "for i, d in enumerate(dataset_names):\n",
    "    data_path = os.path.join(dirpath, d)\n",
    "\n",
    "    # Get dataset dict info\n",
    "    filenames_dict = {}\n",
    "    filenames_dict['filters'] = ['g', 'r', 'i']\n",
    "\n",
    "    # Get each unqiue tract-patch in the data directory\n",
    "    #file = full path name\n",
    "    files = glob.glob(os.path.join(data_path, '*_scarlet_segmask.fits'))\n",
    "    if sampleNumbers != -1:\n",
    "        files = files[:sampleNumbers]\n",
    "    # s = sample name\n",
    "    s = [os.path.basename(f).split('_scarlet_segmask.fits')[0] for f in files]\n",
    "    print(f'Tract-patch List: {s}')\n",
    "    for f in filenames_dict['filters']:\n",
    "        filenames_dict[f] = {}\n",
    "        # List of image files in the dataset\n",
    "        #Yufeng dec/21  [Errno 2] No such file or directory: '/home/shared/hsc/test/G-I-8525-4,5-c5_scarlet_img'\n",
    "        #filenames_dict[f]['img'] = [os.path.join(data_path, f'{f.upper()}-{tract_patch}_scarlet_img.fits') for tract_patch in s]\n",
    "        #Yufeng jan 18 f.upper() indicates filter, tract_patch[1:] removes the default I band in the front\n",
    "        filenames_dict[f]['img'] = [os.path.join(data_path, f.upper() + f'{tract_patch[1:]}_scarlet_img.fits') for tract_patch in s]\n",
    "        # List of mask files in the dataset\n",
    "        #Yufeng jan 18 all mask files are in the I band\n",
    "        filenames_dict[f]['mask'] = [os.path.join(data_path, f'{tract_patch}_scarlet_segmask.fits') for tract_patch in s]\n",
    "        \n",
    "    filenames_dict_list.append(filenames_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ccd77e9-d664-4dc3-8c43-389fe017befc",
   "metadata": {},
   "outputs": [],
   "source": [
    "#number of total samples\n",
    "print('# of train sample: ', len(filenames_dict_list[0]['g']['img']))\n",
    "print('# of test sample: ', len(filenames_dict_list[1]['g']['img']))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bb61c04",
   "metadata": {},
   "source": [
    "For detectron2 to read the data, it must be in a dictionary format.  The function get_astro_dicts reads in the FITS files\n",
    "and formats to a dictionary. \n",
    "\n",
    "However, this step can take a few minutes, and so we recommend only running it once and saving the dictionary data as a json file that can be read in at the beginning of your code.  Check out the `format_hsc_data.ipynb` notebook for how to do that\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df763913-3b26-465a-bddd-c8088b3d065a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_astro_dicts(filename_dict):\n",
    "    \n",
    "    \"\"\"\n",
    "    This needs to be customized to your traning data format\n",
    "    \n",
    "    \"\"\"\n",
    "        \n",
    "    dataset_dicts = []\n",
    "    filters = list(filename_dict.keys())\n",
    "    #yufeng april5: why only 1st filter\n",
    "    f = filename_dict['filters'][0] # Pick the 1st filter for now\n",
    "    \n",
    "    # Filename loop\n",
    "    for idx, (filename_img, filename_mask) in enumerate(zip(filename_dict[f]['img'], filename_dict[f]['mask'])):\n",
    "        record = {}\n",
    "\n",
    "        # Open FITS image of first filter (each should have same shape)\n",
    "        with fits.open(filename_img, memmap=False, lazy_load_hdus=False) as hdul:\n",
    "            height, width = hdul[0].data.shape\n",
    "            \n",
    "        # Open each FITS mask image\n",
    "        with fits.open(filename_mask, memmap=False, lazy_load_hdus=False) as hdul:\n",
    "            hdul = hdul[1:]\n",
    "            sources = len(hdul)\n",
    "            # Normalize data\n",
    "            data = [hdu.data for hdu in hdul]\n",
    "            #category_ids = [hdu.header[\"NEW_ID\"] for hdu in hdul]\n",
    "            category_ids = [0 for hdu in hdul]\n",
    "\n",
    "            ellipse_pars = [hdu.header[\"ELL_PARM\"] for hdu in hdul]\n",
    "            bbox = [list(map(int, hdu.header[\"BBOX\"].split(','))) for hdu in hdul]\n",
    "            area = [hdu.header[\"AREA\"] for hdu in hdul]\n",
    "\n",
    "        # Add image metadata to record (should be the same for each filter)\n",
    "        for f in filename_dict['filters']:\n",
    "            record[f\"filename_{f.upper()}\"] = filename_dict[f]['img'][idx]\n",
    "        # Assign file_name\n",
    "        record[f\"file_name\"] = filename_dict[filename_dict['filters'][0]]['img'][idx]\n",
    "        record[\"image_id\"] = idx\n",
    "        record[\"height\"] = height\n",
    "        record[\"width\"] = width\n",
    "        objs = []\n",
    "\n",
    "        # Generate segmentation masks from model\n",
    "        for i in range(sources):\n",
    "            image = data[i]\n",
    "            # Why do we need this?\n",
    "            if len(image.shape) != 2:\n",
    "                continue\n",
    "            height_mask, width_mask = image.shape\n",
    "            # Create mask from threshold\n",
    "            mask = data[i]\n",
    "            # Smooth mask\n",
    "            #mask = cv2.GaussianBlur(mask, (9,9), 2)\n",
    "            x,y,w,h = bbox[i] # (x0, y0, w, h)\n",
    "\n",
    "            # https://github.com/facebookresearch/Detectron/issues/100\n",
    "            contours, hierarchy = cv2.findContours((mask).astype(np.uint8), cv2.RETR_TREE,\n",
    "                                                        cv2.CHAIN_APPROX_SIMPLE)\n",
    "            segmentation = []\n",
    "            for contour in contours:\n",
    "                # contour = [x1, y1, ..., xn, yn]\n",
    "                contour = contour.flatten()\n",
    "                if len(contour) > 4:\n",
    "                    contour[::2] += (x-w//2)\n",
    "                    contour[1::2] += (y-h//2)\n",
    "                    segmentation.append(contour.tolist())\n",
    "            # No valid countors\n",
    "            if len(segmentation) == 0:\n",
    "                continue\n",
    "\n",
    "            # Add to dict\n",
    "            obj = {\n",
    "                \"bbox\": [x-w//2, y-h//2, w, h],\n",
    "                \"area\": w*h,\n",
    "                \"bbox_mode\": BoxMode.XYWH_ABS,\n",
    "                \"segmentation\": segmentation,\n",
    "                \"category_id\": category_ids[i],\n",
    "                \"ellipse_pars\": ellipse_pars[i]\n",
    "            }\n",
    "            objs.append(obj)\n",
    "        \n",
    "        record[\"annotations\"] = objs\n",
    "        dataset_dicts.append(record)\n",
    "            \n",
    "    return dataset_dicts"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6dfef1a4",
   "metadata": {},
   "source": [
    "Now, we register the dataset following the detectron2 documention."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb2f3c55",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#code snippet for unregistering if you want to change something\n",
    "'''\n",
    "if \"astro_train\" in DatasetCatalog.list():\n",
    "    print('removing astro_train')\n",
    "    DatasetCatalog.remove(\"astro_train\")\n",
    "    #MetadataCatalog.remove(\"astro_train\")\n",
    "    \n",
    "if \"astro_test\" in DatasetCatalog.list():\n",
    "    print('removing astro_test')\n",
    "    DatasetCatalog.remove(\"astro_test\")\n",
    "    #MetadataCatalog.remove(\"astro_test\")\n",
    "\n",
    "if \"astro_val\" in DatasetCatalog.list():\n",
    "    print('removing astro_val')\n",
    "    DatasetCatalog.remove(\"astro_val\")\n",
    "    #MetadataCatalog.remove(\"astro_val\")\n",
    "\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e9bb953",
   "metadata": {},
   "source": [
    "### A note on classes\n",
    "\n",
    "In this demo, we assume one class for all objects.  To see how we assign classes based on external HSC catalogs, check out the `hsc_class_assign.ipynb` notebook "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33d971b2-90a7-4d43-9783-f4ed5f32bb76",
   "metadata": {},
   "outputs": [],
   "source": [
    "DatasetCatalog.register(\"astro_train\", lambda: get_astro_dicts(filenames_dict_list[0]))\n",
    "astrotrain_metadata = MetadataCatalog.get(\"astro_train\").set(thing_classes=[\"object\"])\n",
    "DatasetCatalog.register(\"astro_test\", lambda: get_astro_dicts(filenames_dict_list[1]))\n",
    "astrotest_metadata = MetadataCatalog.get(\"astro_test\").set(thing_classes=[\"object\"])\n",
    "\n",
    "\n",
    "dataset_dicts = {}\n",
    "for i, d in enumerate(dataset_names):\n",
    "    print(f'Loading {d}')\n",
    "    dataset_dicts[d] = get_astro_dicts(filenames_dict_list[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d28831b-1ea3-44e4-ab90-0ee80ae2a93e",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "### Visualize Ground Truth Examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cca74ce-db2e-42cf-bc85-969562719115",
   "metadata": {
    "scrolled": false,
    "tags": []
   },
   "outputs": [],
   "source": [
    "nsample = 1\n",
    "maxInd = sampleNumbers\n",
    "if maxInd == -1: maxInd = 20\n",
    "randInd = np.random.randint(0,maxInd, nsample)\n",
    "fig = plt.figure(figsize=(15,15*nsample*2))\n",
    "i = 0\n",
    "for ind in randInd:\n",
    "    # Need to increase ceil_percentile if the data are saturating!\n",
    "    d = dataset_dicts['train'][ind]\n",
    "    filenames = [d['filename_G'],d['filename_R'],d['filename_I']]\n",
    "    img = toolkit.read_image_hsc(filenames, normalize=\"astrolupton\", stretch=.5, Q=10)\n",
    "    visualizer = Visualizer(img, metadata=astrotrain_metadata)\n",
    "    out = visualizer.draw_dataset_dict(d)\n",
    "    ax1 = plt.subplot(nsample*2, 1, 2*i+1)\n",
    "    ax1.imshow(out.get_image(), origin='upper')\n",
    "    ax1.axis('off')\n",
    "    ax2 = plt.subplot(nsample*2, 1, 2*i+2)\n",
    "    ax2.imshow(img)\n",
    "    ax2.axis('off')\n",
    "    i += 1\n",
    "    \n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4db2ab22",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1, 1, figsize=(6, 5))\n",
    "bins = 25\n",
    "#91,28, 38 are  bad examples\n",
    "d = dataset_dicts['train'][1]\n",
    "filenames=[d['filename_G'],d['filename_R'],d['filename_I']] \n",
    "\n",
    "img = toolkit.read_image_hsc(filenames, normalize=\"astrolupton\", stretch=.5, Q=10)\n",
    "ax.hist(img[:,:,0].flatten(), histtype=\"step\", bins=bins, log=True, color=\"r\", lw=2, zorder=1, label='i');\n",
    "ax.hist(img[:,:,1].flatten(), histtype=\"step\", bins=bins, log=True, color=\"g\", lw=2, linestyle='-.', zorder=2, label='r');\n",
    "ax.hist(img[:,:,2].flatten(), histtype=\"step\", bins=bins, log=True, color=\"b\", lw=2, linestyle='dashed', zorder=3, label='g');\n",
    "ax.set_xlabel('Value', fontsize=20)\n",
    "ax.set_ylabel('Count', fontsize=20)\n",
    "ax.legend(fontsize=18)\n",
    "\n",
    "fig.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8892af2c-0329-4072-979a-79b4b66f63e6",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Data Augmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8e6d44c-45b4-42b7-a7af-516f6a1f631a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from astrodet.detectron import _transform_to_aug\n",
    "def train_mapper(dataset_dict):\n",
    "\n",
    "    dataset_dict = copy.deepcopy(dataset_dict)  # it will be modified by code below\n",
    "    filenames=[dataset_dict['filename_G'],dataset_dict['filename_R'],dataset_dict['filename_I']] \n",
    "    image = toolkit.read_image_hsc(filenames, normalize=\"astrolupton\", stretch=0.5, Q=10)\n",
    "    \n",
    "    augs = detectron_addons.KRandomAugmentationList([\n",
    "            # my custom augs\n",
    "            T.RandomRotation([-90, 90, 180], sample_style='choice'),\n",
    "            T.RandomFlip(prob=0.5),\n",
    "            T.RandomFlip(prob=0.5,horizontal=False,vertical=True),\n",
    "            #detectron_addons.CustomAug(gaussblur,prob=1.0),\n",
    "            #detectron_addons.CustomAug(addelementwise,prob=1.0)\n",
    "            #CustomAug(white),\n",
    "            ],\n",
    "            k=-1,\n",
    "            cropaug=_transform_to_aug(T.CropTransform(image.shape[1]//4,image.shape[0]//4,image.shape[1]//2,image.shape[0]//2))\n",
    "        )\n",
    "\n",
    "    # Data Augmentation\n",
    "    auginput = T.AugInput(image)\n",
    "    # Transformations to model shapes\n",
    "    transform = augs(auginput)\n",
    "    image = torch.from_numpy(auginput.image.copy().transpose(2, 0, 1))\n",
    "    annos = [\n",
    "        utils.transform_instance_annotations(annotation, [transform], image.shape[1:])\n",
    "        for annotation in dataset_dict.pop(\"annotations\")\n",
    "    ]\n",
    "    \n",
    "    instances = utils.annotations_to_instances(annos, image.shape[1:])\n",
    "    instances = utils.filter_empty_instances(instances) \n",
    "    \n",
    "    return {\n",
    "       # create the format that the model expects\n",
    "        \"image\": image,\n",
    "        \"image_shaped\": auginput.image,\n",
    "        \"height\": 1050,\n",
    "        \"width\": 1050,\n",
    "        \"image_id\": dataset_dict[\"image_id\"],\n",
    "        \"instances\": instances,\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ccb9dfb-80f6-4612-927e-5d11fb846331",
   "metadata": {},
   "outputs": [],
   "source": [
    "from detectron2.structures import BoxMode\n",
    "fig, axs = plt.subplots(1,2, figsize=(10*2, 10))\n",
    "\n",
    "dictionary = iter(dataset_dicts['train'])\n",
    "d = next(dictionary)\n",
    "filenames=[d['filename_G'],d['filename_R'],d['filename_I']] \n",
    "\n",
    "img = toolkit.read_image_hsc(filenames, normalize=\"astrolupton\", stretch=0.5, Q=10)\n",
    "visualizer = Visualizer(img, metadata=astrotrain_metadata, scale=1)\n",
    "# Get the ground truth boxes\n",
    "gt_boxes = np.array([a['bbox'] for a in d['annotations']])\n",
    "# Convert to the mode visualizer expects\n",
    "gt_boxes = BoxMode.convert(gt_boxes, BoxMode.XYWH_ABS, BoxMode.XYXY_ABS)\n",
    "out = visualizer.overlay_instances(boxes=gt_boxes)\n",
    "axs[0].imshow(out.get_image())\n",
    "axs[0].axis('off')\n",
    "\n",
    "aug_d = train_mapper(d)\n",
    "img_aug = aug_d[\"image_shaped\"]\n",
    "visualizer = Visualizer(img_aug, metadata=astrotrain_metadata, scale=1)\n",
    "print(img_aug.shape)\n",
    "# Convert to the mode visualizer expects\n",
    "out = visualizer.overlay_instances(boxes=aug_d['instances'].gt_boxes)\n",
    "axs[1].imshow(out.get_image())\n",
    "axs[1].axis('off')\n",
    "fig.tight_layout()\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "901627b5-5ac0-4282-87f0-4a182259075b",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7db17cf5",
   "metadata": {},
   "source": [
    "We prepare for training by intializing a config object and setting hyperparameters.  The we can take the intial weights from the pre-trained models in the model zoo.  For a full list of available config options, check https://detectron2.readthedocs.io/en/latest/modules/config.html\n",
    "\n",
    "This setup is for demo purposes, so it does not follow the full training schedule we use for the paper.  You can check the `train_hsc_primary.py` script for the final training configurations "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07e78059",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "cfg = get_cfg()\n",
    "init_coco_weights = True # Start training from MS COCO weights\n",
    "\n",
    "cfg.merge_from_file(model_zoo.get_config_file(\"COCO-InstanceSegmentation/mask_rcnn_R_101_FPN_3x.yaml\")) # Get model structure\n",
    "cfg.DATASETS.TRAIN = (\"astro_train\") \n",
    "cfg.DATASETS.TEST = (\"astro_test\")\n",
    "\n",
    "cfg.MODEL.RPN.POST_NMS_TOPK_TRAIN=6000\n",
    "cfg.MODEL.ROI_HEADS.POSITIVE_FRACTION = 0.33\n",
    "\n",
    "\n",
    "cfg.DATALOADER.NUM_WORKERS = 2\n",
    "cfg.MODEL.PIXEL_MEAN = [13.49794151,  9.11051305,  5.42995532]\n",
    "    \n",
    "cfg.INPUT.MIN_SIZE_TRAIN = 500\n",
    "cfg.INPUT.MAX_SIZE_TRAIN = 525\n",
    "\n",
    "cfg.MODEL.ANCHOR_GENERATOR.SIZES = [[8, 16, 32, 64, 128]]\n",
    "cfg.SOLVER.IMS_PER_BATCH = 2   # this is images per iteration. 1 epoch is len(images)/(ims_per_batch iterations)\n",
    "\n",
    "cfg.SOLVER.CLIP_GRADIENTS.ENABLED = True\n",
    "# Type of gradient clipping, currently 2 values are supported:\n",
    "# - \"value\": the absolute values of elements of each gradients are clipped\n",
    "# - \"norm\": the norm of the gradient for each parameter is clipped thus\n",
    "#   affecting all elements in the parameter\n",
    "cfg.SOLVER.CLIP_GRADIENTS.CLIP_TYPE = \"norm\"\n",
    "# Maximum absolute value used for clipping gradients\n",
    "# Floating point number p for L-p norm to be used with the \"norm\"\n",
    "# gradient clipping type; for L-inf, please specify .inf\n",
    "cfg.SOLVER.CLIP_GRADIENTS.NORM_TYPE = 5.0\n",
    "\n",
    "\n",
    "\n",
    "e1=200\n",
    "cfg.SOLVER.BASE_LR = 0.001\n",
    "cfg.SOLVER.STEPS = []         \n",
    "cfg.SOLVER.LR_SCHEDULER_NAME = \"WarmupMultiStepLR\"\n",
    "cfg.SOLVER.WARMUP_ITERS = 0\n",
    "cfg.SOLVER.MAX_ITER = e1     # for DefaultTrainer\n",
    "\n",
    "cfg.MODEL.ROI_HEADS.BATCH_SIZE_PER_IMAGE = 512   # faster, and good enough for this toy dataset (default: 512)\n",
    "cfg.MODEL.ROI_HEADS.NUM_CLASSES = 1\n",
    "cfg.OUTPUT_DIR = output_dir\n",
    "cfg.TEST.DETECTIONS_PER_IMAGE = 500\n",
    "\n",
    "cfg.MODEL.BACKBONE.FREEZE_AT = 4\n",
    "\n",
    "if init_coco_weights:\n",
    "    cfg.MODEL.WEIGHTS = model_zoo.get_checkpoint_url(\"COCO-InstanceSegmentation/mask_rcnn_R_101_FPN_3x.yaml\")  # Initialize from MS COCO\n",
    "else:\n",
    "    cfg.MODEL.WEIGHTS = os.path.join(output_dir, 'model_temp.pth')  # Initialize from a local weights\n",
    "\n",
    "os.makedirs(cfg.OUTPUT_DIR, exist_ok=True)\n",
    "model = modeler.build_model(cfg)\n",
    "optimizer = solver.build_optimizer(cfg, model)\n",
    "loader = data.build_detection_train_loader(cfg, mapper=train_mapper)\n",
    "schedulerHook = detectron_addons.CustomLRScheduler(optimizer=optimizer)\n",
    "saveHook = detectron_addons.SaveHook()\n",
    "saveHook.set_output_name(\"model_temp\")\n",
    "hookList = [saveHook,schedulerHook]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b531eed-aff5-4bd0-a13d-e794888f35bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hack if you get SSL certificate error \n",
    "import ssl\n",
    "ssl._create_default_https_context = ssl._create_unverified_context"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14fc25fd",
   "metadata": {},
   "source": [
    "Ignore warnings due to cropping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1c676be",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "try:\n",
    "    # ignore ShapelyDeprecationWarning from fvcore\n",
    "    # This comes from the cropping\n",
    "    from shapely.errors import ShapelyDeprecationWarning\n",
    "    warnings.filterwarnings('ignore', category=ShapelyDeprecationWarning)\n",
    "\n",
    "except:\n",
    "    pass\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bce596b6-a8cf-44e9-8fd7-248ae3471be7",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = toolkit.NewAstroTrainer(model, loader, optimizer, cfg)\n",
    "trainer.register_hooks(hookList)\n",
    "trainer.set_period(10) # print loss every 10 iterations\n",
    "trainer.train(0,e1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d7d9200-a29d-4eb8-8982-8527b74a5bad",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1, 1, figsize=(6, 5))\n",
    "ax.plot(trainer.lossList, label=r'$L_{\\rm{tot}}$')\n",
    "ax.legend(loc='upper right')\n",
    "ax.set_xlabel('training epoch', fontsize=20)\n",
    "ax.set_ylabel('loss', fontsize=20)\n",
    "fig.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c69b958",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99d01641",
   "metadata": {},
   "outputs": [],
   "source": [
    "roi_thresh=0.5\n",
    "nms_thresh = 0.3\n",
    "\n",
    "cfg = get_cfg()\n",
    "cfg.merge_from_file(model_zoo.get_config_file(\"COCO-InstanceSegmentation/mask_rcnn_R_101_FPN_3x.yaml\")) # Get model structure\n",
    "cfg.DATASETS.TRAIN = (\"astro_train\") # Register Metadata # TODO: Should be TRAIN\n",
    "cfg.DATASETS.TEST = (\"astro_test\") # Config calls this TEST, but it should be the val dataset\n",
    "cfg.OUTPUT_DIR = output_dir\n",
    "\n",
    "cfg.MODEL.WEIGHTS = os.path.join(cfg.OUTPUT_DIR, \"model_temp.pth\")  # path to the model we just trained\n",
    "cfg.MODEL.ROI_HEADS.SCORE_THRESH_TEST = 0.2   # todo set a custom testing threshold\n",
    "  \n",
    "cfg.TEST.DETECTIONS_PER_IMAGE = 1000\n",
    "\n",
    "cfg.INPUT.MIN_SIZE_TRAIN = 1025\n",
    "cfg.INPUT.MAX_SIZE_TRAIN = 1050\n",
    "cfg.MODEL.RPN.POST_NMS_TOPK_TEST = 6000  \n",
    "cfg.MODEL.RPN.PRE_NMS_TOPK_TEST = 6000 \n",
    "\n",
    "cfg.MODEL.RPN.BATCH_SIZE_PER_IMAGE = 512\n",
    "cfg.MODEL.ANCHOR_GENERATOR.SIZES = [[8, 16, 32, 64, 128]]\n",
    "\n",
    "\n",
    "cfg.MODEL.ROI_HEADS.SCORE_THRESH_TEST = roi_thresh   # set a custom testing threshold\n",
    "cfg.MODEL.ROI_HEADS.NMS_THRESH_TEST = nms_thresh\n",
    "cfg.MODEL.ROI_HEADS.NUM_CLASSES = 1\n",
    "\n",
    "\n",
    "#predictor = DefaultPredictor(cfg)\n",
    "predictor = toolkit.AstroPredictor(cfg)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c11e59da",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from detectron2.utils.visualizer import ColorMode\n",
    "\n",
    "nsample = 1\n",
    "fig = plt.figure(figsize=(30,15*nsample))\n",
    "\n",
    "for i, d in enumerate(random.sample(dataset_dicts['train'], nsample)):\n",
    "    filenames=[d['filename_G'],d['filename_R'],d['filename_I']] \n",
    "    img = toolkit.read_image_hsc(filenames, normalize=\"astrolupton\", stretch=0.5, Q=10)\n",
    "    print('total instances:', len(d['annotations']))\n",
    "    v0 = Visualizer(img,\n",
    "                   metadata=astrotest_metadata, \n",
    "                   scale=1, \n",
    "                   instance_mode=ColorMode.IMAGE_BW   # remove the colors of unsegmented pixels. This option is only available for segmentation models\n",
    "    )\n",
    "    groundTruth = v0.draw_dataset_dict(d)\n",
    "    \n",
    "    ax1 = plt.subplot(nsample, 2, 2*i+1)\n",
    "    ax1.imshow(groundTruth.get_image())\n",
    "    ax1.axis('off')\n",
    "    \n",
    "    v1 = Visualizer(img,\n",
    "                   metadata=astrotest_metadata, \n",
    "                   scale=1, \n",
    "                   instance_mode=ColorMode.IMAGE_BW   # remove the colors of unsegmented pixels. This option is only available for segmentation models\n",
    "    )\n",
    "    outputs = predictor(img)  # format is documented at https://detectron2.readthedocs.io/tutorials/models.html#model-output-format\n",
    "    out = v1.draw_instance_predictions(outputs[\"instances\"].to(\"cpu\"))\n",
    "    print('detected instances:', len(outputs['instances'].pred_boxes))\n",
    "    print('')\n",
    "    ax1 = plt.subplot(nsample,2,2*i+2)\n",
    "    ax1.imshow(out.get_image())\n",
    "    ax1.axis('off')\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66d34854",
   "metadata": {},
   "source": [
    "### Evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d98ce5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_mapper(dataset_dict, **read_image_args):\n",
    "\n",
    "    dataset_dict = copy.deepcopy(dataset_dict)  # it will be modified by code below\n",
    "    filenames=[d['filename_G'],d['filename_R'],d['filename_I']] \n",
    "    image = toolkit.read_image_hsc(filenames, normalize=\"astrolupton\", stretch=0.5, Q=100)\n",
    "    augs = T.AugmentationList([\n",
    "    ])\n",
    "    # Data Augmentation\n",
    "    auginput = T.AugInput(image)\n",
    "    # Transformations to model shapes\n",
    "    transform = augs(auginput)\n",
    "    image = torch.from_numpy(auginput.image.copy().transpose(2, 0, 1))\n",
    "    annos = [\n",
    "        utils.transform_instance_annotations(annotation, [transform], image.shape[1:])\n",
    "        for annotation in dataset_dict.pop(\"annotations\")\n",
    "    ]\n",
    "    return {\n",
    "       # create the format that the model expects\n",
    "        \"image\": image,\n",
    "        \"image_shaped\": auginput.image,\n",
    "        \"height\": 1050,\n",
    "        \"width\": 1050,\n",
    "        \"image_id\": dataset_dict[\"image_id\"],\n",
    "        \"instances\": utils.annotations_to_instances(annos, image.shape[1:]),\n",
    "        \"annotations\": annos\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eac18542",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "from detectron2.evaluation import inference_on_dataset\n",
    "from detectron2.data import build_detection_test_loader\n",
    "\n",
    "evaluator = toolkit.COCOEvaluatorRecall(\"astro_test\", use_fast_impl=True, allow_cached_coco=False, output_dir=cfg.OUTPUT_DIR)\n",
    "\n",
    "test_loader = data.build_detection_test_loader(cfg, \"astro_test\", mapper=test_mapper)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8836fdaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "results = inference_on_dataset(predictor.model, test_loader, evaluator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecc6c82c-b728-4885-a62e-09ee6c3b5d66",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(results['bbox'].keys())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "213eff62",
   "metadata": {},
   "outputs": [],
   "source": [
    "ap_type = 'bbox' \n",
    "cls_names = ['star', 'galaxy']\n",
    "#take star out april\n",
    "results_per_category = results[ap_type]['results_per_category']\n",
    "\n",
    "#fig, axs = plt.subplots(1, 2, figsize=(15, 5))\n",
    "fig = plt.figure(figsize=(7,4))\n",
    "#axs = plt.subplot(1, 1, figsize=(10, 10))\n",
    "axs = fig.add_subplot(111)\n",
    "#axs = axs.flatten()\n",
    "\n",
    "ious = np.linspace(0.50,0.95,10)\n",
    "colors = plt.cm.viridis(np.linspace(0,1,len(ious)))\n",
    "\n",
    "# Plot precision recall\n",
    "for j, precision_class in enumerate(results_per_category):\n",
    "    precision_shape = np.shape(precision_class)\n",
    "    for i in range(precision_shape[0]):\n",
    "        # precision has dims (iou, recall, cls, area range, max dets)\n",
    "        # area range index 0: all area ranges\n",
    "        # max dets index -1: typically 100 per image\n",
    "        p_dat = precision_class[i, :, j, 0, -1]\n",
    "        # Hide vanishing precisions\n",
    "        mask = (p_dat > 0)\n",
    "        # Only keep first occurance of 0 value in array\n",
    "        mask[np.cumsum(~mask) == 1] = True\n",
    "        p = p_dat[mask]\n",
    "        # Recall points\n",
    "        r = np.linspace(0, 1, len(p))\n",
    "        dr = np.diff(np.linspace(0, 1, len(p_dat)))[0] # i think\n",
    "        # Plot\n",
    "        iou = np.around(ious[i], 2)\n",
    "        AP = 100*np.sum(p*dr)\n",
    "        axs.plot(r, p, label=r'${\\rm{AP}}_{%.2f} = %.1f$' % (iou, AP), color=colors[i], lw=2)\n",
    "        axs.set_xlabel('Recall', fontsize=20)\n",
    "        axs.set_ylabel('Precision', fontsize=20)\n",
    "        axs.set_xlim(0, 1.1)\n",
    "        axs.set_ylim(0, 1.1)\n",
    "        axs.legend(fontsize=10, title=f'{cls_names[j]}', bbox_to_anchor=(1.35, 1.0))\n",
    "        \n",
    "        \n",
    "        #axs[j].plot(r, p, label=r'${\\rm{AP}}_{%.2f} = %.1f$' % (iou, AP), color=colors[i], lw=2)\n",
    "        #axs[j].set_xlabel('Recall', fontsize=20)\n",
    "        #axs[j].set_ylabel('Precision', fontsize=20)\n",
    "        #axs[j].set_xlim(0, 1.1)\n",
    "        #axs[j].set_ylim(0, 1.1)\n",
    "        #axs[j].legend(fontsize=10, title=f'{cls_names[j]}', bbox_to_anchor=(1.35, 1.0))\n",
    "        \n",
    "fig.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9176b2a7",
   "metadata": {},
   "source": [
    "Real data has a lot more variation than simulations and requires more training for the networks to have good evaulation performance.  This demo is just to show how to set up the training.  We encourage you to add object classes, try different contrast scalings, and train for longer!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19e12a60-7b86-48d6-9782-b0462b573a21",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56cf58a9-8593-4b57-bd9e-de59f50434c1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:.conda-deepdisctest]",
   "language": "python",
   "name": "conda-env-.conda-deepdisctest-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
